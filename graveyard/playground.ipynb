{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install tensorflow\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install wandb\n",
    "%pip install os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up WandB\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Variables and Metrics\n",
    "model_version = 'v4'\n",
    "NOISE_SIZE = 100\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 500\n",
    "LEARNING_RATE = 0.0002\n",
    "\n",
    "generator_loss_metric = tf.keras.metrics.Mean()\n",
    "discriminator_loss_metric = tf.keras.metrics.Mean()\n",
    "real_sample_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "fake_sample_accuracy = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Networks\n",
    "generator = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=NOISE_SIZE),\n",
    "    tf.keras.layers.Dense(256),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "    tf.keras.layers.Dense(1024),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "    tf.keras.layers.Dense(784, activation='sigmoid') # sigmoid activation function as want data to be between 0 and 1\n",
    "], name='Generator')\n",
    "\n",
    "# Create the Discriminator Network\n",
    "# This will take in both real and fake data. That means size 784\n",
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=784), \n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "    tf.keras.layers.Dense(256),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # outputs a scalar\n",
    "], name='Discriminator')\n",
    "\n",
    "\n",
    "generator.summary()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Loss Functions and Optimizers\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "def calculate_discriminator_loss(fake_preds, real_preds):\n",
    "    fake_preds_loss = loss_fn(tf.zeros(fake_preds.shape), fake_preds)\n",
    "    real_preds_loss = loss_fn(tf.ones(real_preds.shape), real_preds)\n",
    "    return fake_preds_loss + real_preds_loss\n",
    "\n",
    "def calculate_generator_loss(fake_preds):\n",
    "    return loss_fn(tf.ones(fake_preds.shape), fake_preds)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BATCH_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/derekarmfield/Programming/MachineLearning/Simple-GAN/playground.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/derekarmfield/Programming/MachineLearning/Simple-GAN/playground.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     fake_data \u001b[39m=\u001b[39m model_reference(random_noise_tensor)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/derekarmfield/Programming/MachineLearning/Simple-GAN/playground.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fake_data\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/derekarmfield/Programming/MachineLearning/Simple-GAN/playground.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerateRandomNoise\u001b[39m(batch_size\u001b[39m=\u001b[39mBATCH_SIZE, noise_size\u001b[39m=\u001b[39mNOISE_SIZE):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/derekarmfield/Programming/MachineLearning/Simple-GAN/playground.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     random_noise_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(size\u001b[39m=\u001b[39m(batch_size, noise_size))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/derekarmfield/Programming/MachineLearning/Simple-GAN/playground.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     random_noise_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(random_noise_np)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BATCH_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "# Various Functions for loading the dataset,  genearing fake data, \n",
    "def generateFakeData(model_reference, batch_size=64, noise_size=100):\n",
    "    random_noise_np = np.random.normal(size=(batch_size, noise_size))\n",
    "    random_noise_tensor = tf.convert_to_tensor(random_noise_np)\n",
    "    random_noise_tensor = tf.reshape(random_noise_tensor, [batch_size, noise_size])\n",
    "    fake_data = model_reference(random_noise_tensor)\n",
    "\n",
    "    return fake_data\n",
    "\n",
    "def generateRandomNoise(batch_size=BATCH_SIZE, noise_size=NOISE_SIZE):\n",
    "    random_noise_np = np.random.normal(size=(batch_size, noise_size))\n",
    "    random_noise_tensor = tf.convert_to_tensor(random_noise_np)\n",
    "    return tf.reshape(random_noise_tensor, [batch_size, noise_size])\n",
    "\n",
    "def loadMnistDataset(batch_size=64, input_shape=(28, 28)):\n",
    "    # Load in MNIST dataset as numpy array\n",
    "    (train_X, train_y), (test_X, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    np.random.shuffle(train_X)\n",
    "\n",
    "    # noramlizes and flattens dataset in numpy array\n",
    "    flattened = np.reshape(train_X/255.0, [len(train_X), input_shape[0] * input_shape[1]])\n",
    "\n",
    "    # Loads np array into a Dataset object\n",
    "    train_X_ds = tf.data.Dataset.from_tensor_slices(flattened)\n",
    "\n",
    "    # Returns the flattened, normalized, batched MNIST dataset\n",
    "    return train_X_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Checkpoint Function\n",
    "def checkpointModel(model_reference, epoch, dir='images/v4', batch_size=64, noise_size=100):\n",
    "    path = os.path.join(dir, ('v4-' + epoch))\n",
    "    os.mkdir(path)\n",
    "\n",
    "    generated_imgs_batched = generateFakeData(model_reference, batch_size, noise_size).numpy()\n",
    "\n",
    "    for i, vector in enumerate(generated_imgs_batched):\n",
    "        formated_arr = np.reshape(vector, (28, 28, 1))\n",
    "        plt.imshow(formated_arr, cmap='gray', vmin=0, vmax=1)\n",
    "        plt.savefig(path + '-' + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Training Data\n",
    "training_ds_batched = loadMnistDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # sample minibatch size m of real data\n",
    "    for step, minibatch_real in enumerate(training_ds_batched): \n",
    "\n",
    "        # sample minibatch size m of noise samples\n",
    "        noise = generateRandomNoise(BATCH_SIZE, NOISE_SIZE)\n",
    "        minibatch_fake = generator(noise)\n",
    "\n",
    "        # calculate loss for discriminiator\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_preds = discriminator(minibatch_fake)\n",
    "            real_preds = discriminator(minibatch_real)\n",
    "            discriminator_loss = calculate_discriminator_loss(fake_preds, real_preds)\n",
    "\n",
    "        # calculate gradient for discriminator\n",
    "        discriminator_gradients = tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "        discriminator_loss_metric.update_state(discriminator_loss)\n",
    "        real_sample_accuracy.update_state(tf.ones(real_preds.shape), real_preds)\n",
    "        fake_sample_accuracy.update_state(tf.zeros(fake_preds.shape), fake_preds)\n",
    "        total_sample_accuracy = (real_sample_accuracy.result().numpy() + fake_sample_accuracy.result().numpy()) / 2\n",
    "\n",
    "        # apply gradients\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "\n",
    "        # train generator network\n",
    "        # calculate loss for geneator\n",
    "        noise = generateRandomNoise(BATCH_SIZE, NOISE_SIZE)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # sample minibatch size m of noise samples\n",
    "            minibatch_fake = generator(noise)\n",
    "            fake_preds = discriminator(minibatch_fake)\n",
    "            generator_loss = calculate_generator_loss(fake_preds)\n",
    "\n",
    "        # calculate gradient for generator\n",
    "        generator_gradients = tape.gradient(generator_loss, generator.trainable_variables)\n",
    "        generator_loss_metric.update_state(generator_loss)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "\n",
    "\n",
    "    print('Epoch: %i Generator Loss: %.4f Discriminator Loss: %.4f' %(epoch, generator_loss_metric.result().numpy(), discriminator_loss_metric.result().numpy()))\n",
    "    print('Real Sample Accuracy: %.4f Fake Sample Accuracy: %.4f' %(real_sample_accuracy.result().numpy(), fake_sample_accuracy.result().numpy()))\n",
    "    wandb.log({\n",
    "        \"generator_loss\": generator_loss_metric.result().numpy(), \n",
    "        \"discriminator_loss\": discriminator_loss_metric.result().numpy(),\n",
    "        \"real_sample_accuracy\": real_sample_accuracy.result().numpy(), \n",
    "        \"fake_sample_accuracy\": fake_sample_accuracy.result().numpy(),\n",
    "        \"total_sample_accuracy\": (total_sample_accuracy), \n",
    "    })\n",
    "    generator_loss_metric.reset_states()\n",
    "    discriminator_loss_metric.reset_states()\n",
    "    real_sample_accuracy.reset_states()\n",
    "    fake_sample_accuracy.reset_states()\n",
    "    total_sample_accuracy = 0 # this is purely book keeping. not functional\n",
    "\n",
    "    if epoch % 50 == 0 and not epoch == 0: \n",
    "        generator.save('savedModels/generator-' + model_version + '/epoch-%i' %epoch)\n",
    "        discriminator.save('savedModels/discriminator-' + model_version + '/epoch-%i' %epoch)\n",
    "\n",
    "\n",
    "    checkpointModel(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY TO TERMINATE TRAINING\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final save of generator and discriminator\n",
    "generator.save('savedModels/generator-' + model_version + '/final')\n",
    "discriminator.save('savedModels/discriminator-' + model_version + '/final')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('MedicalClassificationML-tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a18bcef3509e3fe6906f1751551d33c3fd853060613b86c696e737e046fa81c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
